#!/usr/bin/env python3
"""
Analisi quantitativa delle carte informatici
Genera statistiche 0-100 per ogni caratteristica usando CLIP + scoring quantitativo
"""

import sys
import json
from pathlib import Path
from typing import Dict, List, Tuple
import glob

# Setup path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from dotenv import load_dotenv
from quantitative_scoring import convert_to_quantitative_scores

# Load environment
load_dotenv()


def load_vocabulary(vocab_file: str) -> Dict:
    """Carica vocabolario JSON"""
    with open(vocab_file) as f:
        return json.load(f)


def get_scientist_images(cards_dir: str) -> List[Tuple[str, str]]:
    """
    Trova tutte le immagini degli informatici
    Returns: List of (name, image_path)
    """
    images = []
    for img_path in glob.glob(f"{cards_dir}/*.jpeg"):
        filename = Path(img_path).stem
        # Converti nome file in nome leggibile
        # "Ada_Lovelace__computer_scientist" -> "Ada Lovelace"
        name = filename.split("__")[0].replace("_", " ")
        images.append((name, img_path))

    return sorted(images)


def analyze_with_clip(
    image_path: str, prompts: List[str], clip_model_tuple=None
) -> List[float]:
    """
    Analizza immagine con CLIP
    Returns: List of similarity scores (0-1) per ogni prompt
    """
    try:
        from PIL import Image
        import torch
        import clip

        # Load model if not provided
        if clip_model_tuple is None:
            from embed import initialize_clip_model

            model, preprocess, device = initialize_clip_model()
        else:
            model, preprocess, device = clip_model_tuple

        # Load and preprocess image
        image = Image.open(image_path).convert("RGB")
        image_input = preprocess(image).unsqueeze(0).to(device)

        # Tokenize all prompts at once
        text_tokens = clip.tokenize(prompts).to(device)

        # Extract features
        with torch.no_grad():
            image_features = model.encode_image(image_input)
            text_features = model.encode_text(text_tokens)

            # Normalize features
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)

            # Compute similarities (cosine similarity)
            similarities = (image_features @ text_features.T).squeeze(0)

        # Convert to Python list
        similarities = similarities.cpu().numpy().tolist()

        return similarities

    except Exception as e:
        print(f"âš ï¸  CLIP analysis failed: {e}")
        print("   Using fallback: simulated scores based on prompt position")
        import random

        # Use weighted random to simulate progression
        # Lower prompts = lower scores, higher prompts = higher scores
        return [
            random.uniform(0.1 + i * 0.1, 0.25 + i * 0.15) for i in range(len(prompts))
        ]


def create_card_display(
    name: str, scores: Dict[str, int], rank: int, total: int
) -> str:
    """Crea rappresentazione ASCII della carta"""

    # Trova caratteristica piÃ¹ alta
    max_char = max(scores.items(), key=lambda x: x[1])

    # Crea carta
    card = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  #{rank}/{total}  {"â­" * min(5, rank)}
â•‘
â•‘  {name[:38].center(38)}
â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
"""

    # Aggiungi caratteristiche
    for char_name, score in scores.items():
        # Crea barra di progresso
        bar_length = 20
        filled = int(score / 100 * bar_length)
        bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)

        # Evidenzia la migliore
        prefix = "ğŸ†" if char_name == max_char[0] else "  "

        # Nome caratteristica formattato
        display_name = char_name.replace("_", " ").title()[:20]

        card += f"â•‘ {prefix} {display_name:<20} {score:3d} {bar} â•‘\n"

    card += "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    return card


def main():
    print("\n" + "=" * 60)
    print("  NIMITZ - Analisi Quantitativa Carte Informatici")
    print("=" * 60)

    # Load vocabulary
    vocab_file = "vocabolario_informatici_game.json"
    print(f"\nğŸ“– Caricamento vocabolario: {vocab_file}")
    vocab = load_vocabulary(vocab_file)

    characteristics = vocab["characteristics"]
    print(f"âœ“ Caricate {len(characteristics)} caratteristiche:")
    for char_name in characteristics.keys():
        display_name = char_name.replace("_", " ").title()
        print(f"   â€¢ {display_name}")

    # Find images
    cards_dir = "./informatici_cards"
    print(f"\nğŸ–¼ï¸  Cerca immagini in: {cards_dir}")
    scientists = get_scientist_images(cards_dir)
    print(f"âœ“ Trovate {len(scientists)} immagini")

    # Initialize CLIP model once (reuse for all images)
    print("\nğŸ¤– Inizializzazione CLIP model...")
    try:
        from embed import initialize_clip_model

        clip_model = initialize_clip_model()
        print("âœ“ CLIP model caricato")
    except Exception as e:
        print(f"âš ï¸  CLIP non disponibile: {e}")
        print("   Uso fallback con punteggi simulati")
        clip_model = None

    # Analyze each scientist
    print("\nğŸ”¬ Analisi in corso...\n")

    all_cards = []

    for i, (name, img_path) in enumerate(scientists, 1):
        print(f"[{i}/{len(scientists)}] Analizzo: {name}")

        card_data = {"name": name, "image": img_path, "scores": {}}

        # Analyze each characteristic
        for char_name, prompts in characteristics.items():
            # Get CLIP similarities
            similarities_list = analyze_with_clip(img_path, prompts, clip_model)

            # Convert to numpy array
            import numpy as np

            similarities = np.array(similarities_list)

            # Convert to 0-100 score
            result = convert_to_quantitative_scores(
                similarities=similarities,
                characteristic_name=char_name,
                prompts=prompts,
            )

            # Extract score from result dict
            score = int(result["score"])

            card_data["scores"][char_name] = score

            # Show inline
            display_name = char_name.replace("_", " ").title()
            print(f"   {display_name}: {score}/100")

        all_cards.append(card_data)
        print()

    # Calculate rankings
    print("\n" + "=" * 60)
    print("  CLASSIFICA TOP 3 PER CARATTERISTICA")
    print("=" * 60 + "\n")

    for char_name in characteristics.keys():
        display_name = char_name.replace("_", " ").title()
        print(f"\nğŸ† {display_name.upper()}")
        print("-" * 60)

        # Sort by this characteristic
        ranked = sorted(all_cards, key=lambda x: x["scores"][char_name], reverse=True)

        for rank, card in enumerate(ranked[:3], 1):
            medal = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][rank - 1]
            score = card["scores"][char_name]
            print(f"  {medal} {rank}. {card['name']:<30} {score:3d}/100")

    # Display sample cards
    print("\n" + "=" * 60)
    print("  ESEMPI DI CARTE")
    print("=" * 60)

    # Show top 3 overall (by average score)
    for card in all_cards:
        card["avg_score"] = sum(card["scores"].values()) / len(card["scores"])

    ranked_overall = sorted(all_cards, key=lambda x: x["avg_score"], reverse=True)

    for i, card in enumerate(ranked_overall[:5], 1):
        print(
            create_card_display(
                name=card["name"], scores=card["scores"], rank=i, total=len(all_cards)
            )
        )
        print()

    # Save results
    output_file = "informatici_cards_analysis.json"
    with open(output_file, "w") as f:
        json.dump(all_cards, f, indent=2)

    print("\n" + "=" * 60)
    print(f"âœ… Analisi completata!")
    print(f"   Risultati salvati in: {output_file}")
    print(f"   Totale carte: {len(all_cards)}")
    print("=" * 60)


if __name__ == "__main__":
    main()
