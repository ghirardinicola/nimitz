# ğŸ‰ Configurazione Completata: NIMITZ + vLLM Leitha

## Riepilogo Setup

âœ… **Server vLLM:** `https://agent-codeai.leitha.servizi.gr-u.it/v1`  
âœ… **Modello:** `Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8`  
âœ… **API Key:** `anything`  
âœ… **Status:** Server attivo e raggiungibile

---

## ğŸš€ Uso Rapido

### Opzione 1: Script di Setup Automatico (CONSIGLIATO)

```bash
cd /Users/nic/prj/nimitz
source ./setup_vllm_leitha.sh
```

Lo script:
- âœ… Configura automaticamente le variabili d'ambiente
- âœ… Testa la connessione al server
- âœ… Ti chiede se vuoi salvare la configurazione in `~/.bashrc` o `~/.zshrc`

### Opzione 2: Configurazione Manuale

```bash
# Aggiungi queste righe a ~/.bashrc o ~/.zshrc
export VLLM_BASE_URL="https://agent-codeai.leitha.servizi.gr-u.it/v1"
export VLLM_API_KEY="anything"
export VLLM_MODEL="Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"

# Ricarica il profilo
source ~/.bashrc  # o ~/.zshrc se usi zsh
```

### Opzione 3: Uso Temporaneo (Solo Sessione Corrente)

```bash
export VLLM_BASE_URL="https://agent-codeai.leitha.servizi.gr-u.it/v1"
export VLLM_API_KEY="anything"
export VLLM_MODEL="Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"
```

---

## ğŸ“ Esempi di Utilizzo

### 1. Discovery Base con vLLM

```bash
nimitz retrieve discover "italian baseball players" \
    -o players.txt \
    --llm-provider vllm
```

**Cosa succede:**
1. ğŸŒ Cerca sul web "italian baseball players"
2. ğŸ¤– Usa il server vLLM Leitha per filtrare i nomi reali
3. ğŸ’¬ Ti chiede conferma interattiva
4. ğŸ’¾ Salva i risultati in `players.txt`

### 2. Discovery con Auto-Detection

Se hai configurato `VLLM_BASE_URL`, vLLM viene usato automaticamente:

```bash
nimitz retrieve discover "famous scientists" -o scientists.txt
```

(Non serve specificare `--llm-provider vllm`, viene rilevato automaticamente!)

### 3. Discovery + Auto-Retrieve Immagini

```bash
nimitz retrieve discover "renaissance painters" \
    -o painters.txt \
    --llm-provider vllm \
    --auto \
    --preset art
```

**Workflow completo:**
1. ğŸ” Discover â†’ filtra con vLLM
2. ğŸ“¸ Scarica automaticamente immagini da Unsplash
3. ğŸ’¾ Salva tutto in cartelle organizzate

### 4. Workflow Completo: Discover â†’ Generate Cards

```bash
# Step 1: Discover entities
nimitz retrieve discover "italian football players" \
    -o players.txt \
    --llm-provider vllm \
    --auto

# Step 2: Generate trading cards
nimitz generate \
    -i nimitz_output \
    -o trading_cards \
    -c "Name,Position,Goals,Assists,Team"
```

---

## ğŸ§ª Test della Configurazione

### Test 1: Verifica Variabili d'Ambiente

```bash
echo "VLLM_BASE_URL: $VLLM_BASE_URL"
echo "VLLM_API_KEY: $VLLM_API_KEY"
echo "VLLM_MODEL: $VLLM_MODEL"
```

**Output atteso:**
```
VLLM_BASE_URL: https://agent-codeai.leitha.servizi.gr-u.it/v1
VLLM_API_KEY: anything
VLLM_MODEL: Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8
```

### Test 2: Verifica Connessione Server

```bash
curl -s "https://agent-codeai.leitha.servizi.gr-u.it/v1/models" \
    -H "Authorization: Bearer anything" | jq .
```

**Output atteso:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8",
      "object": "model",
      ...
    }
  ]
}
```

### Test 3: Test NIMITZ Help

```bash
nimitz retrieve discover --help | grep vllm
```

**Output atteso:**
```
--llm-provider {auto,anthropic,gemini,openai,vllm}
```

### Test 4: Test Completo (Richiede Server Attivo)

```bash
# Test con dati reali
nimitz retrieve discover "test query" \
    -o test_output.txt \
    --llm-provider vllm \
    --max-results 5
```

---

## ğŸ”§ Troubleshooting

### Problema: "VLLM_BASE_URL environment variable not set"

**Soluzione:**
```bash
# Verifica che le variabili siano impostate
echo $VLLM_BASE_URL

# Se vuoto, impostale di nuovo
export VLLM_BASE_URL="https://agent-codeai.leitha.servizi.gr-u.it/v1"
export VLLM_API_KEY="anything"
export VLLM_MODEL="Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"
```

### Problema: "Connection refused" o timeout

**Cause possibili:**
1. Server vLLM offline
2. Problemi di rete/VPN
3. Firewall che blocca la connessione

**Debug:**
```bash
# Test connessione base
curl -v "https://agent-codeai.leitha.servizi.gr-u.it/v1/models" \
    -H "Authorization: Bearer anything"

# Se fallisce, verifica:
# - Sei connesso alla VPN? (se necessaria)
# - Il server Ã¨ attivo?
# - Hai accesso a internet?
```

### Problema: Risposte lente o incomplete

Il modello Qwen3-Coder-30B Ã¨ grande e potente, ma potrebbe essere piÃ¹ lento di GPT/Claude.

**Tips:**
- Riduci `--max-results` per query piÃ¹ veloci
- Il primo run Ã¨ piÃ¹ lento (cold start del modello)
- Successive query sono piÃ¹ veloci (cache)

### Problema: "LLM filtering failed"

Se vedi questo warning, NIMITZ fallback alla lista originale (senza filtering).

**Possibili cause:**
- Risposta malformata dal modello
- Timeout della richiesta
- Errore di parsing JSON

**Debug:** Controlla i log completi per vedere l'errore esatto.

---

## ğŸ“Š Confronto Provider

### vLLM Leitha vs Cloud APIs

| Feature | vLLM Leitha | Claude | Gemini | GPT-4 |
|---------|-------------|--------|--------|-------|
| **Costo** | Gratis (interno) | $$$$ | $$$ | $$$$ |
| **Privacy** | âœ… Interno | âš ï¸ Cloud | âš ï¸ Cloud | âš ï¸ Cloud |
| **VelocitÃ ** | ğŸŸ¡ Media | ğŸŸ¢ Veloce | ğŸŸ¢ Veloce | ğŸŸ¡ Media |
| **QualitÃ ** | ğŸŸ¢ Buona | ğŸŸ¢ Ottima | ğŸŸ¢ Ottima | ğŸŸ¢ Ottima |
| **Uptime** | ğŸŸ¡ Dipende | ğŸŸ¢ 99.9% | ğŸŸ¢ 99.9% | ğŸŸ¢ 99.9% |

**Quando usare vLLM Leitha:**
- âœ… Vuoi tenere i dati interni
- âœ… Non vuoi costi API
- âœ… Il server Ã¨ sempre disponibile
- âœ… La qualitÃ  Ã¨ sufficiente per il task

**Quando usare Cloud APIs:**
- âœ… Massima qualitÃ  richiesta
- âœ… Uptime critico
- âœ… vLLM non disponibile

---

## ğŸ¯ PrioritÃ  Auto-Detection

Quando usi `--llm-provider auto`, NIMITZ cerca in questo ordine:

1. **vLLM** (se `VLLM_BASE_URL` impostato) â† HAI PRIORITÃ€!
2. **Claude** (se `ANTHROPIC_API_KEY` impostato)
3. **Gemini** (se `GEMINI_API_KEY` impostato)
4. **OpenAI** (se `OPENAI_API_KEY` impostato)

Quindi con la tua configurazione, **vLLM Leitha viene sempre usato** di default!

Per forzare un altro provider:
```bash
# Usa Claude anche se vLLM Ã¨ configurato
nimitz retrieve discover "query" --llm-provider anthropic
```

---

## ğŸ“š File di Riferimento

- **Setup automatico:** `setup_vllm_leitha.sh`
- **Test connessione:** `test_vllm_leitha.py`
- **Guida completa vLLM:** `docs/VLLM_SETUP.md`
- **Quickstart generale:** `QUICKSTART_VLLM.md`
- **Note implementazione:** `IMPLEMENTATION_NOTES.md`

---

## âœ… Checklist Setup Completo

- [x] Variabili d'ambiente configurate
- [x] Server vLLM raggiungibile e testato
- [x] Modello corretto (Qwen3-Coder-30B-A3B-Instruct-FP8)
- [x] NIMITZ aggiornato con supporto vLLM
- [x] CLI mostra opzione "vllm"
- [x] Auto-detection funzionante
- [ ] Test con query reale â† **PROSSIMO PASSO!**

---

## ğŸš€ Prossimo Passo: Test Reale

Prova subito con una query vera:

```bash
# Setup (se non giÃ  fatto)
source setup_vllm_leitha.sh

# Test!
nimitz retrieve discover "giocatori baseball italiani" \
    -o test_baseball.txt \
    --llm-provider vllm \
    --max-results 10
```

Dovresti vedere:
1. ğŸŒ Ricerca web
2. ğŸ¤– "Using LLM to filter results..."
3. ğŸ“‹ Lista nomi filtrati
4. ğŸ’¬ Richiesta di conferma
5. âœ… File salvato

---

**Tutto configurato! Buon lavoro con NIMITZ + vLLM Leitha! ğŸ‰**
